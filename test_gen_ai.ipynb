{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sentencepiece_model_pb2' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/bokey/HDD_internal_500G1/open_source_projects/sapeint NER/auto_ner/test_gen_ai.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load the tokenizer and model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgoogle/flan-t5-base\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Labeled examples\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1854\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1852\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1854\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1855\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1856\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1857\u001b[0m     init_configuration,\n\u001b[1;32m   1858\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1859\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1860\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1861\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1862\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1863\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[1;32m   1864\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1865\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2017\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2017\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   2018\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   2019\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   2020\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2021\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2022\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:194\u001b[0m, in \u001b[0;36mT5Tokenizer.__init__\u001b[0;34m(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, legacy, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extra_ids \u001b[39m=\u001b[39m extra_ids\n\u001b[0;32m--> 194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_spm_processor()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:200\u001b[0m, in \u001b[0;36mT5Tokenizer.get_spm_processor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    199\u001b[0m     sp_model \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 200\u001b[0m     model_pb2 \u001b[39m=\u001b[39m import_protobuf()\n\u001b[1;32m    201\u001b[0m     model \u001b[39m=\u001b[39m model_pb2\u001b[39m.\u001b[39mModelProto\u001b[39m.\u001b[39mFromString(sp_model)\n\u001b[1;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegacy:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:40\u001b[0m, in \u001b[0;36mimport_protobuf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m sentencepiece_model_pb2_new \u001b[39mas\u001b[39;00m sentencepiece_model_pb2\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m sentencepiece_model_pb2\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sentencepiece_model_pb2' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Labeled examples\n",
    "examples = [\n",
    "    (\"The circuit_component on the PCB was malfunctioning.\", \"Labels: circuit_component\"),\n",
    "    (\"The resistor R1 is connected to the circuit.\", \"Labels: resistor\")\n",
    "]\n",
    "\n",
    "# Create a few-shot NER prompt\n",
    "prompt = \"\\n\".join([f\"NER: {example[0]} {example[1]}\" for example in examples])\n",
    "\n",
    "# Tokenize and generate\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, max_length=100, num_return_sequences=len(examples))\n",
    "\n",
    "# Decode and print the generated outputs\n",
    "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "for i, generated_text in enumerate(generated_texts):\n",
    "    print(f\"Example {i + 1}: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_return_sequences has to be 1, but is 2 when doing greedy search.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/bokey/HDD_internal_500G1/open_source_projects/sapeint NER/auto_ner/test_gen_ai.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Tokenize and generate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids, max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(examples))\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Decode and print the generated outputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/bokey/HDD_internal_500G1/open_source_projects/sapeint%20NER/auto_ner/test_gen_ai.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m generated_texts \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mdecode(output, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/transformers/generation_utils.py:984\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m is_greedy_gen_mode:\n\u001b[1;32m    983\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 984\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         )\n\u001b[1;32m    988\u001b[0m     \u001b[39m# greedy search\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[1;32m    990\u001b[0m         input_ids,\n\u001b[1;32m    991\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    999\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_return_sequences has to be 1, but is 2 when doing greedy search."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Load a pre-trained tokenizer that's compatible with T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Labeled examples\n",
    "examples = [\n",
    "    (\"The circuit_component on the PCB was malfunctioning.\", \"circuit_component\"),\n",
    "    (\"The resistor R1 is connected to the circuit.\", \"resistor\")\n",
    "]\n",
    "\n",
    "# Create a few-shot NER prompt\n",
    "prompt = \"\\n\".join([f\"NER: {example[0]} Labels: {example[1]}\" for example in examples])\n",
    "\n",
    "# Tokenize and generate\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=100, num_return_sequences=len(examples))\n",
    "\n",
    "# Decode and print the generated outputs\n",
    "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "for i, generated_text in enumerate(generated_texts):\n",
    "    print(f\"Example {i + 1}: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 1.8.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /home/bokey/anaconda3/envs/gpu/lib/python3.8/site-packages\n",
      "Requires: typing-extensions, numpy\n",
      "Required-by: torchvision, torchaudio\n"
     ]
    }
   ],
   "source": [
    "! pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.32.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/bokey/anaconda3/envs/gpu/lib/python3.8/site-packages\n",
      "Requires: filelock, requests, huggingface-hub, pyyaml, packaging, regex, numpy, tokenizers, safetensors, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preheat the oven to 375 degrees F. In a large bowl, combine the olive']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NER: The circuit_component on the PCB was malfunctioning. Labels: circuit_component\\nNER: The resistor R1 is connected to the circuit. Labels: resistor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a few-shot NER prompt\n",
    "prompt = \"\\n\".join([f\"NER: {example[0]} Labels: {example[1]}\" for example in examples])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bokey/anaconda3/envs/gpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: capacitor\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load a pre-trained tokenizer that's compatible with T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Labeled examples\n",
    "examples = [\n",
    "    (\"The LED board on the PCB was malfunctioning.\", \" LED board\"),\n",
    "    (\"The resistor R1 is connected to the circuit.\", \"resistor\")\n",
    "]\n",
    "\n",
    "# Query text\n",
    "query = \"The capacitor of this radio needs to be replaced.\"\n",
    "\n",
    "# Create a few-shot NER prompt\n",
    "prompt = \"\\n\".join([f\"NER: {example[0]} Labels: {example[1]}\" for example in examples])\n",
    "prompt += f\"\\n{query} Labels:\"\n",
    "\n",
    "# Tokenize and generate\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Decode and print the generated output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
